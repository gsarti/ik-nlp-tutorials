{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls8l9AYCmBSh"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W6T_Advanced_Prompting_Generation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9323,
     "status": "ok",
     "timestamp": 1739179691011,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "L1zp2I-QjrZ3",
    "outputId": "47139f2e-735c-4810-c9f4-3396af4b42d5"
   },
   "outputs": [],
   "source": [
    "# Run in Notebook to install local packages\n",
    "!pip install torch transformers bitsandbytes accelerate rank_bm25 outlines datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxT0HqKgk8Z6"
   },
   "source": [
    "# Advanced Prompting and Generation with ü§ó Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeChp_lUkusF"
   },
   "source": [
    "*This notebook is based on the [HuggingFace Developer Guide](https://huggingface.co/docs/transformers/chat_templating) as well as this [introduction on agents](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak0Hvb2wqLxK"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdmGsiR2qVqP"
   },
   "source": [
    "Let's load the model and the tokenizer that we will be using for this tutorial. We will be using the Qwen2.5-1.5B-Instruct model from the ü§ó [HuggingFace Hub](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct), as it's lightweight and can use tools, which we will showcase later in this tutorial. We will load it using 8-bit quantization, to further reduce the VRAM requirements while maintaining relatively good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 13533,
     "status": "ok",
     "timestamp": 1739179826663,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "w4kuRvWbqEL4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "checkpoint = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Configure 8-bit quantization. We use this to save VRAM, as we don't have a lot available.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True  # Enables 8-bit quantization\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=bnb_config,  # Apply BitsAndBytesConfig\n",
    "    device_map=\"cuda\"   # Assign to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyXs-bcEr3IS"
   },
   "source": [
    "## Chat Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7GbRWgnppwY"
   },
   "source": [
    "An increasingly common use case for LLMs is chat. In a chat context, rather than continuing a single string of text (as is the case with a standard language model), the model instead continues a conversation that consists of one or more messages, each of which includes a role, like ‚Äúuser‚Äù or ‚Äúassistant‚Äù, as well as message text.\n",
    "\n",
    "Much like tokenization, different models expect very different input formats for chat. This is the reason chat templates exist. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.\n",
    "\n",
    "Let‚Äôs make this concrete with a quick example using the previously loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1739178734764,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "rUfCFFWeo-9y",
    "outputId": "47082e90-dc55-44bc-fbf8-296edc658e9a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing great. How can I help you today?<|im_end|>\\n<|im_start|>user\\nI'd like to show off how chat templating works!<|im_end|>\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UymH2tmptfNb"
   },
   "source": [
    "Notice how the tokenizer has added the control tokens <code> <|im_start|> </code> and <code><|im_end|></code> to indicate the start and end of messages, and the entire chat is condensed into a single string. If we use <code>tokenize=True</code>, which is the default setting, that string will also be tokenized for us.\n",
    "\n",
    "Now, if we had used a different model, such as [Mistral's 7B model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1), the output would have been different:\n",
    "\n",
    "\n",
    "```\n",
    "<s> [INST] You are a helpful assistant\n",
    "Hello, how are you? [/INST] I'm doing great. How can I help you today?</s>\n",
    "[INST] I'd like to show off how chat templating works! [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0zRlgTMzkYv"
   },
   "source": [
    "\n",
    "The two models were trained with totally different chat formats. Without chat templates, you would have to write manual formatting code for each model, and it‚Äôs very easy to make minor errors that hurt performance! Chat templates handle the details of formatting for you, allowing you to write universal code that works for any model.\n",
    "\n",
    "\n",
    "\n",
    "> **Good to Know**: Models fine-tuned using the same base model could still use different chat formats!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1fNLzfO0tN7"
   },
   "source": [
    "### How do I use chat templates?\n",
    "\n",
    "As you can see in the example above, chat templates are easy to use. Simply build a list of messages, with role and content keys, and then pass it to the <code>apply_chat_template()</code> method. Once you do that, you‚Äôll get output that‚Äôs ready to go! When using chat templates as input for model generation, it‚Äôs also a good idea to use <code>add_generation_prompt=True</code> to add a generation prompt.\n",
    "\n",
    "Here‚Äôs an example of preparing input for <code>model.generate()</code>, using the model we previously loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1739179838876,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "SOFbJlkL0tpQ",
    "outputId": "ac080c9e-089a-45a6-a2bb-8c8c5d57f762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(tokenizer.decode(tokenized_chat[0])) # This will yield a string in the input format that our model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS3be5MK1Nbt"
   },
   "source": [
    "Now that our input is formatted correctly for our model, we can use the model to generate a response to the user‚Äôs question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12377,
     "status": "ok",
     "timestamp": 1739179852474,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "nDQEYeRg1M5z",
    "outputId": "2be461ac-0082-4cf7-cce4-8b7994455ffe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ahoy there! That's quite an odd question, matey. Helicopters are not edible fare for humans, and even if they were, it would be more than just a few to swallow all at once. Let me tell you, sailors, our food is mostly fruits, veggies, fish, and other maritime delicacies. Now, what sea do ye wish to sail upon?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgdKSGZs2dLx"
   },
   "source": [
    "> **Good to Know**: The <code>add_generation_prompt</code> argument tells the template to add tokens that indicate the start of a bot response, by simply appending <code><|im_start|>assistant</code>. This ensures that when the model generates text it will write a bot response instead of doing something unexpected, like continuing the user‚Äôs message. Remember, chat models are still just language models - they‚Äôre trained to continue text, and chat is just a special kind of text to them! You need to guide them with appropriate control tokens, so they know what they‚Äôre supposed to be doing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTjfVzIfbGgL"
   },
   "source": [
    "### What does <code>continue_final_message</code> do?\n",
    "When passing a list of messages to <code>apply_chat_template</code>, you can choose to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply extend the final message when it begins to generate text. This is useful for ‚Äúprefilling‚Äù the model‚Äôs response.\n",
    "\n",
    "Here‚Äôs an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1716,
     "status": "ok",
     "timestamp": 1738936797769,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "TyCnhTmqbGBR",
    "outputId": "fe1397f2-efcd-4591-d60f-d045f4a84984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you format the answer in JSON?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"Qwen\", \"role\": \"helpful assistant\"}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, continue_final_message=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozP4AyV5baG6"
   },
   "source": [
    "The model will generate text that continues the JSON string, rather than starting a new message. This approach can be very useful for improving the accuracy of the model‚Äôs instruction-following when you know how you want it to start its replies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DIYr9_Sbb-4"
   },
   "source": [
    "> **Good to Know**: Because <code>add_generation_prompt</code> adds the tokens that start a new message, and <code>continue_final_message</code> removes any end-of-message tokens from the final message, it does not make sense to use them together. As a result, you‚Äôll get an error if you try!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvmVPMrX4Wrj"
   },
   "source": [
    "## Tool use / function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNLiPe724tYV"
   },
   "source": [
    "‚ÄúTool use‚Äù LLMs can choose to call functions as external tools before generating an answer. When passing tools to a tool-use model, you can simply pass a list of functions to the <code>tools</code> argument:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruXISzkf42Bn"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def current_time():\n",
    "    \"\"\"Get the current local time as a string.\"\"\"\n",
    "    return str(datetime.now())\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"\n",
    "    A function that multiplies two numbers\n",
    "\n",
    "    Args:\n",
    "        a: The first number to multiply\n",
    "        b: The second number to multiply\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = {\n",
    "    \"current_time\": current_time,\n",
    "    \"multiply\": multiply\n",
    "}\n",
    "model_input = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=list(tools.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdCsPfaX49cK"
   },
   "source": [
    "> **Good to Know**:  In order for this to work correctly, you should write your functions in the format above, so that they can be parsed correctly as tools. Specifically, you should follow these rules:\n",
    "1. The function should have a descriptive name\n",
    "2. Every argument must have a type hint (e.g., \"a: float\")\n",
    "3. The function must have a docstring in the standard Google style (in other words, an initial function description followed by an <code>Args:</code> block that describes the arguments, unless the function does not have any arguments)\n",
    "4. Do not include types in the Args: block. In other words, write <code>a: The first number to multiply</code>, not <code>a (int): The first number to multiply</code>. Type hints should go in the function header instead.\n",
    "5. The function can have a return type and a <code>Returns:</code> block in the docstring. However, these are optional because most tool-use models ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jQ8ZSZO0boO"
   },
   "source": [
    "### Passing tool results to the model\n",
    "The sample code above is enough to list the available tools for your model, but what happens if it wants to actually use one? If that happens, you should:\n",
    "\n",
    "1. Parse the model‚Äôs output to get the tool name(s) and arguments.\n",
    "2. Add the model‚Äôs tool call(s) to the conversation.\n",
    "3. Call the corresponding function(s) with those arguments.\n",
    "4. Add the result(s) to the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS0KoiMHaLwg"
   },
   "source": [
    "### A complete tool use example\n",
    "Let‚Äôs walk through a tool use example, step by step.\n",
    "\n",
    "Next, let‚Äôs define a list of tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pRXkYTn0a4R"
   },
   "outputs": [],
   "source": [
    "def get_current_temperature(location: str, unit: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units, as a float.\n",
    "    \"\"\"\n",
    "    return 22.  # A real function should probably actually get the temperature!\n",
    "\n",
    "def get_current_wind_speed(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current wind speed in km/h at a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current wind speed at the given location in km/h, as a float.\n",
    "    \"\"\"\n",
    "    return 6.  # A real function should probably actually get the wind speed!\n",
    "\n",
    "tools = {\n",
    "    \"get_current_temperature\": get_current_temperature,\n",
    "    \"get_current_wind_speed\": get_current_wind_speed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1hFE54P0jSG"
   },
   "source": [
    "Now, let‚Äôs set up a conversation for our bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfw66XmC0d8m"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location. Use the tools provided.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now in Celsius?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdLbqjmC0nBW"
   },
   "source": [
    "Now, let‚Äôs apply the chat template and generate a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4353,
     "status": "ok",
     "timestamp": 1738936802147,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "VgzkdyiP0lNd",
    "outputId": "36546a4e-f0e9-4e36-cb24-582a39921bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_call>\n",
      "{\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n",
      "</tool_call><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, tools=list(tools.values()), add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "out = model.generate(**inputs, max_new_tokens=128)\n",
    "decoded_output = tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):])\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWkIW6iAPTuT"
   },
   "source": [
    "Hopefully, the model produced a valid function call. Let's try to parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1738936802162,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "HxsbreOSLjCy",
    "outputId": "cd557702-00fb-4743-f27a-a40be162cec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Function Call: {'name': 'get_current_temperature', 'arguments': {'location': 'Paris, France', 'unit': 'celsius'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "# Extract function calls using regex\n",
    "tool_call_match = re.search(r\"<tool_call>(.*?)</tool_call>\", decoded_output, re.DOTALL)\n",
    "\n",
    "if tool_call_match:\n",
    "    try:\n",
    "        # Strip the matched string and load it as JSON\n",
    "        tool_call = json.loads(tool_call_match.group(1).strip()) # Convert to Dictionary\n",
    "        print(\"\\nExtracted Function Call:\", tool_call)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing tool call:\", tool_call_match.group(1))\n",
    "else:\n",
    "    print(\"No tool call found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQCcPwfJ07lz"
   },
   "source": [
    "The model has called the function with valid arguments, in the format requested by the function docstring. It has inferred that we‚Äôre most likely referring to the Paris in France, and it remembered to display the temperature using Celsius.\n",
    "\n",
    "Next, let‚Äôs get the result of the function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYvL2mCkPwmt"
   },
   "outputs": [],
   "source": [
    "func_name = tool_call[\"name\"]\n",
    "args = tool_call[\"arguments\"]\n",
    "\n",
    "result = None\n",
    "if func_name in tools:\n",
    "    result = tools[func_name](**args)\n",
    "else:\n",
    "    print(f\"Function {func_name} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeJ9yAPB1DM_"
   },
   "source": [
    "Now that we have the result, we can append it to the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3J0wuuI1GYI"
   },
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"tool\", \"name\": func_name, \"content\": str(result)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVn1Jd-U1P7p"
   },
   "source": [
    "Finally, let‚Äôs let the assistant read the function outputs and continue chatting with the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1738936803958,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "9zUdtGrw1PrO",
    "outputId": "d88710b8-ff03-4897-8782-5a89e4bf2a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Paris is 22.0¬∞C.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, tools=list(tools.values()), add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "out = model.generate(**inputs, max_new_tokens=128)\n",
    "print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_s4Od2V1TXi"
   },
   "source": [
    "Although this was a simple demo with dummy tools and a single call, the same technique works with multiple real tools and longer conversations. This can be a powerful way to extend the capabilities of conversational agents with real-time information, computational tools like calculators, or access to large databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc0nm9EixzXF"
   },
   "source": [
    "## Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22k4JAhizxTD"
   },
   "source": [
    "In this section we cover two advanced prompting techniques that can significantly improve the performance of any Large Language Model, especially for reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oemcmjzzu5M"
   },
   "source": [
    "### Few-shot prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVAQQCbC0Jp8"
   },
   "source": [
    "The basic prompts in the sections above are the examples of ‚Äúzero-shot‚Äù prompts, meaning, the model has been given instructions and context, but no examples with solutions. LLMs that have been fine-tuned on instruction datasets, generally perform well on such ‚Äúzero-shot‚Äù tasks. However, you may find that your task has more complexity or nuance, and, perhaps, you have some requirements for the output that the model doesn‚Äôt catch on just from the instructions. In this case, you can try the technique called few-shot prompting.\n",
    "\n",
    "In few-shot prompting, we provide examples in the prompt giving the model more context to improve the performance. The examples condition the model to generate the output following the patterns in the examples.\n",
    "\n",
    "Here‚Äôs an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1931,
     "status": "ok",
     "timestamp": 1739180688729,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "q3c2d2uRx822",
    "outputId": "2aee70d9-db4e-4db3-ce12-3236649b9565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Parse the date from the following text:\n",
      "Text: The first human went into space and orbited the Earth on April 12, 1961.\n",
      "Date: 12/04/1961\n",
      "Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\n",
      "Date:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "September 28, 1960<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Parse the date from the following text:\n",
    "Text: The first human went into space and orbited the Earth on April 12, 1961.\n",
    "Date: 12/04/1961\n",
    "Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\n",
    "Date:\"\"\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt = True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128, do_sample=False, temperature=None, top_p=None, top_k=None)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvmX_hSH1uvo"
   },
   "source": [
    "In the above code snippet we used a single example to demonstrate the desired output to the model, so this can be called a ‚Äúone-shot‚Äù prompting. However, depending on the task complexity you may need to use more than one example.\n",
    "\n",
    "> **Good to Know**: Limitations of the few-shot prompting technique:\n",
    "1. While LLMs can pick up on the patterns in the examples, these technique doesn‚Äôt work well on complex reasoning tasks\n",
    "2. Few-shot prompting requires creating lengthy prompts. Prompts with large number of tokens can increase computation and latency. There‚Äôs also a limit to the length of the prompts.\n",
    "3. Sometimes when given a number of examples, models can learn patterns that you didn‚Äôt intend them to learn, e.g. that the third movie review is always negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5oFz5xdIqn8"
   },
   "source": [
    "### Structured Generation with Outlines\n",
    "\n",
    "In the few-shot example above, the generation returns a date, but it does not respect the format that was provided in the example. It is however possible to force LMs to generate according to pre-defined constraints, simply by building a tree of valid tokens at every step and considering only those as valid when sampling the solution. The [Outlines](https://dottxt-ai.github.io/outlines) library provides a simple API with many wrappers for this purpose, supporting structures like JSONs, [Pydantic](https://docs.pydantic.dev/latest/) Python classes and Regexes. Let's try again the previous example with Regex structured generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1889,
     "status": "ok",
     "timestamp": 1739180694352,
     "user": {
      "displayName": "Gabriele Sarti",
      "userId": "14923386733505457429"
     },
     "user_tz": -60
    },
    "id": "aspNDB8sKV5s",
    "outputId": "7bed9056-1b4c-406c-f5ce-ea81a7cf44e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/09/1960\n"
     ]
    }
   ],
   "source": [
    "from outlines import models, generate\n",
    "from outlines.generate.api import GenerationParameters, SamplingParameters\n",
    "\n",
    "#(0[1-9]|[12][0-9]|3[01]) - Day: Either 01-09, 10-29, or 30-31\n",
    "#/ - Literal forward slash\n",
    "#(0[1-9]|1[0-2]) - Month: Either 01-09 or 10-12\n",
    "#/ - Literal forward slash\n",
    "#([12][0-9]{3}) - Year: 1000-2999\n",
    "dateformat_regex = r\"(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[0-2])/([12][0-9]{3})\"\n",
    "\n",
    "# Wrap the model in outlines\n",
    "outlines_model = models.Transformers(model, tokenizer)\n",
    "\n",
    "generator = generate.regex(outlines_model, dateformat_regex)\n",
    "\n",
    "# Input should be in text format, not vectors (automatically processed by Outlines)\n",
    "tokenized_chat_txt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt = True)\n",
    "\n",
    "# See GenerationParameters, SamplingParameters for allowed generation args\n",
    "answer = generator(tokenized_chat_txt, max_tokens=128)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuccigIJQGSG"
   },
   "source": [
    "More info about structured generation can be found in the [Outlines documentation](https://dottxt-ai.github.io/outlines/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S66a-d2b0DNJ"
   },
   "source": [
    "### Chain of Thought (CoT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWOh28Kn0PgS"
   },
   "source": [
    "Chain-of-thought (CoT) prompting is a technique that nudges a model to produce intermediate reasoning steps thus improving the results on complex reasoning tasks.\n",
    "\n",
    "There are two ways of steering a model to producing the reasoning steps:\n",
    "\n",
    "1. few-shot prompting by illustrating examples with detailed answers to questions, showing the model how to work through a problem.\n",
    "2. by instructing the model to reason by adding phrases like ‚ÄúLet‚Äôs think step by step‚Äù or ‚ÄúTake a deep breath and work through the problem step by step.‚Äù\n",
    "\n",
    "Here are two examples, one with and one without CoT (using the 2nd method above combined with <code>continue_final_message</code> to further incentivise the model to 'reason').\n",
    ">Remember that <code>continue_final_message</code> and <code>add_generation_prompt</code> cannot be used together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4823,
     "status": "ok",
     "timestamp": 1738940025074,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "dBG-bEjo5GNF",
    "outputId": "9b0db4b6-1448-46e2-afbc-891ca618af9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "There are 4 apples on the table. Jack eats one. Elen eats a pear, and adds an apple to the table. How many apples are on the table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Jack eats one apple, so there is still 1 apple left on the table.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "## No Chain of Thought\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"There are 4 apples on the table. Jack eats one. Elen eats a pear, and adds an apple to the table. How many apples are on the table?\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7449,
     "status": "ok",
     "timestamp": 1738940032538,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "4p4UoYRJ0DNK",
    "outputId": "3eed1f2d-9145-44fe-ec9c-273c342a6e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "There are 4 apples on the table. Jack eats one. Elen eats a pear, and adds an apple to the table. How many apples are on the table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's think step by step: Initially there were 4 apples on the table.\n",
      "Jack ate one, so now there are 3 apples left.\n",
      "Elen added one apple back to the table, making it 4 again.\n",
      "\n",
      "Therefore, there are still 4 apples on the table.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"There are 4 apples on the table. Jack eats one. Elen eats a pear, and adds an apple to the table. How many apples are on the table?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Let's think step by step:\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, continue_final_message=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgQMQrYg6LS5"
   },
   "source": [
    "As a last example on this topic, you can of course combine few-shot prompting with CoT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8632,
     "status": "ok",
     "timestamp": 1738940407347,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "t49CJfDV6S5A",
    "outputId": "96b60066-bb4f-42a0-a7cb-3f7980386207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "    Question: There are 4 apples on the table. Jack eats one. Elen eats a pear, and adds an apple to the table. How many apples are on the table?\n",
      "    Reasoning: Let's think step by step: Initially there were 4 apples on the table. Jack ate one, so now there are 3 apples left. Elen added one apple back to the table, making it 4 again.\n",
      "    Answer: 4\n",
      "    Question: There are 6 oranges on the table. Sam takes two. Lily eats a banana and puts one orange on the table. How many oranges are on the table now?\n",
      "    <|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reasoning: Let's think step by step: Initially, there were 6 oranges on the table. Sam took two, leaving 4 oranges. Lily put one orange back on the table, increasing the count to 5 oranges.\n",
      "\n",
      "Answer: 5<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "  Question: There are 4 apples on the table. Jack eats one. Elen eats a pear, and adds an apple to the table. How many apples are on the table?\n",
    "  Reasoning: Let's think step by step: Initially there were 4 apples on the table. Jack ate one, so now there are 3 apples left. Elen added one apple back to the table, making it 4 again.\n",
    "  Answer: 4\n",
    "  Question: There are 6 oranges on the table. Sam takes two. Lily eats a banana and puts one orange on the table. How many oranges are on the table now?\n",
    "    \"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Reasoning: Let's think step by step:\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, continue_final_message=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzbiQZUbcwRQ"
   },
   "source": [
    "## Best practices of LLM prompting\n",
    "In this section of the guide we have compiled a list of best practices that tend to improve the prompt results:\n",
    "\n",
    "1. When choosing the model to work with, the latest and most capable models are likely to perform better.\n",
    "2. Start with a simple and short prompt, and iterate from there.\n",
    "3. Put the instructions at the beginning of the prompt, or at the very end. When working with large context, models apply various optimizations to prevent Attention complexity from scaling quadratically. This may make a model more attentive to the beginning or end of a prompt than the middle.\n",
    "4. Clearly separate instructions from the text they apply to.\n",
    "5. Be specific and descriptive about the task and the desired outcome - its format, length, style, language, etc.\n",
    "6. Avoid ambiguous descriptions and instructions.\n",
    "7. Favor instructions that say ‚Äúwhat to do‚Äù instead of those that say ‚Äúwhat not to do‚Äù.\n",
    "8. ‚ÄúLead‚Äù the output in the right direction by writing the first word (or even begin the first sentence for the model).\n",
    "9. Use advanced techniques like [Few-shot prompting](https://huggingface.co/docs/transformers/tasks/prompting#few-shot-prompting) and [Chain-of-thought](https://huggingface.co/docs/transformers/tasks/prompting#chain-of-thought).\n",
    "10. Test your prompts with different models to assess their robustness.\n",
    "11. Version and track the performance of your prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZub2J9Kb4-t"
   },
   "source": [
    "## Retrieval-augmented generation\n",
    "\n",
    "‚ÄúRetrieval-augmented generation‚Äù or ‚ÄúRAG‚Äù LLMs can search a corpus of documents for information before responding to a query. This allows models to vastly expand their knowledge base beyond their limited context size.\n",
    "Below we present a minimal example, using a list of four facts as documents. There are four main steps:\n",
    "1. Document tokenisation (using <code>BM25Okapi</code>).\n",
    "2. Retrieval of most relevant documents (top 2 in this case, using <code>bm25.get_top_n</code>).\n",
    "3. Use of <code>apply_chat_template</code> to prepare the prompt, inserting the context (retrieved documents) in the user message.\n",
    "4. Generate the answer, as we usually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2225,
     "status": "ok",
     "timestamp": 1738936806186,
     "user": {
      "displayName": "L. Zotos",
      "userId": "12433501933183882926"
     },
     "user_tz": -60
    },
    "id": "RXgnDm20b7GY",
    "outputId": "b6c91d43-6515-47ce-dadf-bfb9a9e77391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Context: Shakespeare wrote many famous plays.\n",
      "The Eiffel Tower is located in Paris, France.\n",
      "Query: What is the most famous landmark in Paris?<|im_end|>\n",
      "\n",
      "Response: system\n",
      "You are a helpful assistant\n",
      "user\n",
      "Context: Shakespeare wrote many famous plays.\n",
      "The Eiffel Tower is located in Paris, France.\n",
      "Query: What is the most famous landmark in Paris?\n",
      "system\n",
      "The most famous landmark in Paris is the Eiffel Tower.\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Sample document collection\n",
    "documents = [\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"The capital of Germany is Berlin.\",\n",
    "    \"Shakespeare wrote many famous plays.\",\n",
    "    \"The Pacific Ocean is the largest ocean on Earth.\"\n",
    "]\n",
    "\n",
    "# Query for the model to answer\n",
    "query = 'What is the most famous landmark in Paris?'\n",
    "\n",
    "# Tokenize documents for BM25\n",
    "tokenized_docs = [doc.split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Retrieve top_k passages using BM25\n",
    "top_k = 2\n",
    "tokenized_query = query.split()\n",
    "top_docs = bm25.get_top_n(tokenized_query, documents, n=top_k)\n",
    "\n",
    "# Apply chat template\n",
    "context = \"\\n\".join(top_docs)\n",
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "  {\"role\": \"user\", \"content\": f\"Context: {context}\\nQuery: {query}\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "# Generate response\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n",
    "output_ids = model.generate(input_ids, max_length=100)\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITiRZuOzoQdI"
   },
   "source": [
    "## LLM Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWJ0per8mrlR"
   },
   "source": [
    "### ü§î What are agents?\n",
    "\n",
    "Any efficient system using AI will need to provide LLMs some kind of access to the real world: for instance the possibility to call a search tool to get external information, or to act on certain programs in order to solve a task. In other words, LLMs should have *agency*. Agentic programs are the gateway to the outside world for LLMs.\n",
    "\n",
    "> **AI Agents are programs where LLM outputs control the workflow.**\n",
    "\n",
    "\n",
    "Any system leveraging LLMs will integrate the LLM outputs into code. The influence of the LLM‚Äôs input on the code workflow is the level of agency of LLMs in the system.\n",
    "\n",
    "Note that with this definition, ‚Äúagent‚Äù is not a discrete, 0 or 1 definition: instead, ‚Äúagency‚Äù evolves on a continuous spectrum, as you give more or less power to the LLM on your workflow.\n",
    "\n",
    "This agentic system runs in a loop, executing a new action at each step (the action can involve calling some pre-determined tools), until its observations make it apparent that a satisfactory state has been reached to solve the given task. Here‚Äôs an example of how a multi-step agent can solve a simple math question:\n",
    "\n",
    "![LLM Agents Introduction](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt6scnsdmrZY"
   },
   "source": [
    "### ‚úÖ When to use agents / ‚õî when to avoid them\n",
    "\n",
    "Agents are useful when you need an LLM to determine the workflow of an app. But they‚Äôre often overkill. The question is: do I really need flexibility in the workflow to efficiently solve the task at hand? If the pre-determined workflow falls short too often, that means you need more flexibility. Let‚Äôs take an example: say you‚Äôre making an app that handles customer requests on a surfing trip website.\n",
    "\n",
    "You could know in advance that the requests will belong to either of 2 buckets (based on user choice), and you have a predefined workflow for each of these 2 cases.\n",
    "\n",
    "1. Want some knowledge on the trips? ‚áí give them access to a search bar to search your knowledge base\n",
    "2. Wants to talk to sales? ‚áí let them type in a contact form.\n",
    "\n",
    "If that deterministic workflow fits all queries, by all means just code everything! This will give you a 100% reliable system with no risk of error introduced by letting unpredictable LLMs meddle in your workflow. For the sake of simplicity and robustness, it‚Äôs advised to regularize towards not using any agentic behaviour.\n",
    "\n",
    "But what if the workflow can‚Äôt be determined that well in advance?\n",
    "\n",
    "For instance, a user wants to ask:\n",
    "\n",
    "> \"I can come on Monday, but I forgot my passport so risk being delayed to Wednesday, is it possible to take me and my stuff to surf on Tuesday morning, with a cancellation insurance?\"\n",
    "\n",
    "This question hinges on many factors, and probably none of the predetermined criteria above will suffice for this request.\n",
    "\n",
    "If the pre-determined workflow falls short too often, that means you need more flexibility.\n",
    "\n",
    "That is where an agentic setup helps.\n",
    "\n",
    "In the above example, you could just make a multi-step agent that has access to a weather API for weather forecasts, Google Maps API to compute travel distance, an employee availability dashboard and a RAG system on your knowledge base.\n",
    "\n",
    "Until recently, computer programs were restricted to pre-determined workflows, trying to handle complexity by piling up if/else switches. They focused on extremely narrow tasks, like ‚Äúcompute the sum of these numbers‚Äù or ‚Äúfind the shortest path in this graph‚Äù. But actually, most real-life tasks, like our trip example above, do not fit in pre-determined workflows. Agentic systems open up the vast world of real-world tasks to programs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXibiBKiwbtY"
   },
   "source": [
    "### Code agents\n",
    "\n",
    "In a multi-step agent, at each step, the LLM can write an action, in the form of some calls to external tools. A common format (used by Anthropic, OpenAI, and many others) for writing these actions is generally different shades of ‚Äúwriting actions as a JSON of tools names and arguments to use, which you then parse to know which tool to execute and with which arguments‚Äù.\n",
    "\n",
    "[Multiple research papers](https://huggingface.co/papers/2411.01747) have shown that having the tool calling LLMs in code is much better.\n",
    "\n",
    "The reason for this simply that we crafted our code languages specifically to be the best possible way to express actions performed by a computer. If JSON snippets were a better expression, JSON would be the top programming language and programming would be hell on earth.\n",
    "\n",
    "The figure below, taken from [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030), illustrate some advantages of writing actions in code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CE4OuoA1wJ4V"
   },
   "source": [
    "![Code not json](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174-Pdx-1bu9"
   },
   "source": [
    "# Read More:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AI5Ru2H8A52"
   },
   "source": [
    "\n",
    "#### On Chat Templates:\n",
    "1. [Automated Pipeline for Chat](https://huggingface.co/docs/transformers/chat_templating#is-there-an-automated-pipeline-for-chat)\n",
    "2. [What does ‚Äúcontinue_final_message‚Äù do?](https://huggingface.co/docs/transformers/chat_templating#what-does-continuefinalmessage-do)\n",
    "3. [Using Chat Templates In Training](https://huggingface.co/docs/transformers/chat_templating#can-i-use-chat-templates-in-training)\n",
    "4. [Understanding tool schemas\n",
    "](https://huggingface.co/docs/transformers/chat_templating#understanding-tool-schemas)\n",
    "5. [Prompting vs Fine-Tuning](https://huggingface.co/docs/transformers/tasks/prompting#prompting-vs-fine-tuning)\n",
    "\n",
    "#### On Agents:\n",
    "1. [SmolAgents](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents#why-smolagents-)\n",
    "2. [Agents Example: Text to SQL](https://huggingface.co/docs/smolagents/examples/text_to_sql)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CXibiBKiwbtY"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
