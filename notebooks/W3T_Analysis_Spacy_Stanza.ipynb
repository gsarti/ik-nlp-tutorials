{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W3T_Analysis_Spacy_Stanza.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in Colab to install local packages\n",
    "!pip install spacy stanza\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy and Stanza\n",
    "\n",
    "*This tutorial is based on the [Advanced NLP with spaCy](https://course.spacy.io/en) course, the [spaCy documentation](https://spacy.io/usage/linguistic-features) and the [Stanza documentation](https://stanfordnlp.github.io/stanza/).*\n",
    "\n",
    "Processing raw text intelligently is difficult: most words are rare, and it‚Äôs common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it‚Äôs possible to solve some problems starting from only the raw characters, it‚Äôs usually better to use linguistic knowledge to add useful information. That‚Äôs exactly what spaCy is designed to do: \n",
    "\n",
    "- You create a pipeline, or load one of the ones available into an instance, usually called simply `nlp`.\n",
    "- You put in raw text, and get back a [Doc](https://spacy.io/api/doc) object, that comes with a variety of annotations.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://spacy.io/images/pipeline.svg\" alt=\"Visualizing a nlp pipeline\" style=\"width: 50%\">\n",
    "</div>\n",
    "\n",
    "Docs are containers for [Token](https://spacy.io/api/token) objects, which can also be subsetted in [Span](https://spacy.io/api/span) objects. It contains the raw text, the part of speech tag, the dependency relation, the named entity tag, and more.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://course.spacy.io/doc_span.png\" alt=\"Visualizing tokens, docs and spans\" style=\"width: 40%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 13:13:42.395971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-23 13:13:42.395991: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: []\n",
      "Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Text\tLemma\tUPOS\tXPOS\tDEP\tShape\tIsAlpha\tIsStop\tIsPunct\tIsAscii\n",
      "--------------------------------------------------------------------------------\n",
      "Apple\tApple\tPROPN\tNNP\tnsubj\tXxxxx\tTrue\tFalse\tFalse\tTrue\n",
      "is\tbe\tAUX\tVBZ\taux\txx\tTrue\tTrue\tFalse\tTrue\n",
      "looking\tlook\tVERB\tVBG\tROOT\txxxx\tTrue\tFalse\tFalse\tTrue\n",
      "at\tat\tADP\tIN\tprep\txx\tTrue\tTrue\tFalse\tTrue\n",
      "buying\tbuy\tVERB\tVBG\tpcomp\txxxx\tTrue\tFalse\tFalse\tTrue\n",
      "U.K.\tU.K.\tPROPN\tNNP\tdobj\tX.X.\tFalse\tFalse\tFalse\tTrue\n",
      "startup\tstartup\tVERB\tVBD\tdep\txxxx\tTrue\tFalse\tFalse\tTrue\n",
      "for\tfor\tADP\tIN\tprep\txxx\tTrue\tTrue\tFalse\tTrue\n",
      "$\t$\tSYM\t$\tquantmod\t$\tFalse\tFalse\tFalse\tTrue\n",
      "1\t1\tNUM\tCD\tcompound\td\tFalse\tFalse\tFalse\tTrue\n",
      "billion\tbillion\tNUM\tCD\tpobj\txxxx\tTrue\tFalse\tFalse\tTrue\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load an blank pipeline for English, i.e. only basic lexical information\n",
    "# like tokenization, is_number, is_alpha, is_punct, etc.\n",
    "nlp = spacy.blank('en')\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# Load a small pipeline for English. This includes also a POS tagger, a dependency\n",
    "# parser and a named entity recognizer trained on labeled data.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "def print_attributes(doc_or_span):\n",
    "    print(\"Text\\tLemma\\tUPOS\\tXPOS\\tDEP\\tShape\\tIsAlpha\\tIsStop\\tIsPunct\\tIsAscii\\n\" + \"-\"*80)\n",
    "    for token in doc_or_span:\n",
    "        print(\n",
    "            f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\\t\"\n",
    "            f\"{token.dep_}\\t{token.shape_}\\t{token.is_alpha}\\t\"\n",
    "            f\"{token.is_stop}\\t{token.is_punct}\\t{token.is_ascii}\"\n",
    "        )\n",
    "\n",
    "print_attributes(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes extracted:\n",
    "\n",
    "- **Text:** The original word text.\n",
    "\n",
    "- **Lemma:** The base form of the word.\n",
    "\n",
    "- **POS:** The simple UPOS part-of-speech tag.\n",
    "\n",
    "- **Tag:** The detailed part-of-speech tag.\n",
    "\n",
    "- **Dep:** Syntactic dependency, i.e. the relation between tokens.\n",
    "\n",
    "- **Shape:** The word shape ‚Äì capitalization, punctuation, digits.\n",
    "\n",
    "- **is alpha:** Is the token an alpha character?\n",
    "\n",
    "- **is stop:** Is the token part of a stop list, i.e. the most common words of the language?\n",
    "\n",
    "- **is punct:** Is the token a punctuation mark?\n",
    "\n",
    "- **is_ascii:** Is the token part of the ASCII charset?\n",
    "\n",
    "If some tags may look mysterious for the non-linguists among you, the `spacy.explain` tool can help in getting a better grasp at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantmod: modifier of quantifier\n",
      "dobj: direct object\n",
      "VBZ: verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "print(\"quantmod:\", spacy.explain(\"quantmod\"))\n",
    "print(\"dobj:\", spacy.explain(\"dobj\"))\n",
    "print(\"VBZ:\", spacy.explain(\"VBZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tUPOS\tXPOS\tDEP\tShape\tIsAlpha\tIsStop\tIsPunct\tIsAscii\n",
      "--------------------------------------------------------------------------------\n",
      "is\tbe\tAUX\tVBZ\taux\txx\tTrue\tTrue\tFalse\tTrue\n",
      "looking\tlook\tVERB\tVBG\tROOT\txxxx\tTrue\tFalse\tFalse\tTrue\n",
      "at\tat\tADP\tIN\tprep\txx\tTrue\tTrue\tFalse\tTrue\n",
      "buying\tbuy\tVERB\tVBG\tpcomp\txxxx\tTrue\tFalse\tFalse\tTrue\n"
     ]
    }
   ],
   "source": [
    "# A slice from the doc is a Span object\n",
    "\n",
    "span = doc[1:5]\n",
    "print_attributes(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "The [Lemmatizer](https://spacy.io/api/lemmatizer) is a pipeline component that provides lookup and rule-based lemmatization methods in a configurable component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule\n",
      "['I', 'be', 'read', 'the', 'paper', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# English pipelines include a rule-based lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "if \"lemmatizer\" not in nlp.pipe_names:\n",
    "    config = {\"mode\": \"rule\"}\n",
    "    lemmatizer = nlp.add_pipe(\"lemmatizer\", config=config)\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)\n",
    "\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule-based deterministic lemmatizer maps the surface form to a lemma in light of the previously assigned coarse-grained part-of-speech and morphological information, without consulting the context of the token. The rule-based lemmatizer also accepts list-based exception files. For English, these are acquired from [WordNet](https://wordnet.princeton.edu/).\n",
    "\n",
    ">**üí° Interesting Fact** spaCy v3 introduced new, experimental, machine learning-based lemmatizer that posts accuracies above 95% for many languages. These lemmatizer learns to predict lemmatization rules from a corpus of examples and removes the need to write an exhaustive set of per-language lemmatization rules. See more in this blog post: [Neural edit-tree lemmatization for spaCy](https://explosion.ai/blog/edit-tree-lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the task of splitting a text into meaningful segments, called tokens. The input to the tokenizer is a unicode text, and the output is a Doc object. To construct a Doc object, you need a [Vocab](https://spacy.io/api/vocab) instance, a sequence of word strings, and optionally a sequence of spaces booleans, which allow you to maintain alignment of the tokens into the original string.\n",
    "\n",
    "spaCy‚Äôs tokenization is non-destructive, which means that you‚Äôll always be able to reconstruct the original input from the tokenized output. Whitespace information is preserved in the tokens and no information is added or removed during tokenization.\n",
    "\n",
    "The process of splitting a text into tokens in spaCy is started with a **whitespace tokenization**, and then a series of splitting steps are applied to prefixes, suffixes, infixes and other exceptions (e.g. N.Y. and U.K. are kept as a single token).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg\" alt=\"Tokenization example with spaCy\" style=\"width: 60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [spaCy Documentation](https://spacy.io/usage/linguistic-features#tokenization) for more details on how to define custom rules and exceptions.\n",
    "\n",
    "### Creating a custom tokenizer\n",
    "\n",
    "We are going now to demonstrate how a custom tokenizer can be used to replace the default tokenizer in a spaCy pipeline. We are going to create a class `WhitespaceTokenizer` and replace the original English tokenizer with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before substitution: ['What', \"'s\", 'happened', 'to', 'me', '?', 'he', 'thought', '.', 'It', 'was', \"n't\", 'a', 'dream', '.']\n",
      "After substitution: [\"What's\", 'happened', 'to', 'me?', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        spaces = [True] * len(words)\n",
    "        # Avoid zero-length tokens\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"\":\n",
    "                words[i] = \" \"\n",
    "                spaces[i] = False\n",
    "        # Remove the final trailing space\n",
    "        if words[-1] == \" \":\n",
    "            words = words[0:-1]\n",
    "            spaces = spaces[0:-1]\n",
    "        else:\n",
    "           spaces[-1] = False   \n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"What's happened to me? he thought. It wasn't a dream.\")\n",
    "print(\"Before substitution:\", [token.text for token in doc])\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "doc = nlp(\"What's happened to me? he thought. It wasn't a dream.\")\n",
    "print(\"After substitution:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-tokenized text\n",
    "\n",
    "spaCy generally assumes by default that your data is raw text. However, sometimes your data is partially annotated, e.g. with pre-existing tokenization, part-of-speech tags, etc. The most common situation is that you have pre-defined tokenization. If you have a list of strings, you can create a Doc object directly. Optionally, you can also specify a list of boolean values, indicating whether each word is followed by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tUPOS\tXPOS\tDEP\tShape\tIsAlpha\tIsStop\tIsPunct\tIsAscii\n",
      "--------------------------------------------------------------------------------\n",
      "Hello\t\t\t\t\tXxxxx\tTrue\tFalse\tFalse\tTrue\n",
      ",\t\t\t\t\t,\tFalse\tFalse\tTrue\tTrue\n",
      "world\t\t\t\t\txxxx\tTrue\tFalse\tFalse\tTrue\n",
      "!\t\t\t\t\t!\tFalse\tFalse\tTrue\tTrue\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text\tLemma\tUPOS\tXPOS\tDEP\tShape\tIsAlpha\tIsStop\tIsPunct\tIsAscii\n",
      "--------------------------------------------------------------------------------\n",
      "Hello\thello\t\tNN\tnsubj\tXxxxx\tTrue\tFalse\tFalse\tTrue\n",
      ",\t,\t\tNN\tROOT\t,\tFalse\tFalse\tTrue\tTrue\n",
      "world\tworld\t\tNN\tdobj\txxxx\tTrue\tFalse\tFalse\tTrue\n",
      "!\t!\t\tNN\tpunct\t!\tFalse\tFalse\tTrue\tTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gsarti/projects/venv/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "words = [\"Hello\", \",\", \"world\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print_attributes(doc)\n",
    "print(\"\\n\\n\\n\")\n",
    "doc = nlp.get_pipe(\"tagger\")(doc)\n",
    "doc = nlp.get_pipe(\"parser\")(doc)\n",
    "doc = nlp.get_pipe(\"lemmatizer\")(doc)\n",
    "print_attributes(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning tokenization\n",
    "\n",
    "spaCy‚Äôs tokenization is non-destructive and uses language-specific rules optimized for compatibility with treebank annotations. Other tools and resources can sometimes tokenize things differently ‚Äì for example, `\"I'm\"` ‚Üí `[\"I\", \"'\", \"m\"]` instead of `[\"I\", \"'m\"]`.\n",
    "\n",
    "In situations like that, you often want to align the tokenization so that you can merge annotations from different sources together, or take vectors predicted by a pretrained BERT model and apply them to spaCy tokens. spaCy‚Äôs [`Alignment`](https://spacy.io/api/example#alignment-object) object allows the one-to-one mappings of token indices in both directions as well as taking into account indices where multiple tokens align to one single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> b, lengths: [1 1 1 1 1 1 1 1]\n",
      "a -> b, mapping: [0 1 2 3 4 4 5 6]\n",
      "b -> a, lengths: [1 1 1 1 2 1 1]\n",
      "b -> a, mappings: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "from spacy.training import Alignment\n",
    "\n",
    "other_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'\", \"s\", \"podcasts\", \".\"]\n",
    "spacy_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'s\", \"podcasts\", \".\"]\n",
    "align = Alignment.from_strings(other_tokens, spacy_tokens)\n",
    "print(f\"a -> b, lengths: {align.x2y.lengths}\")  # array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "print(f\"a -> b, mapping: {align.x2y.data}\")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to \"'s\"\n",
    "print(f\"b -> a, lengths: {align.y2x.lengths}\")  # array([1, 1, 1, 1, 2, 1, 1])   : the token \"'s\" refers to two tokens\n",
    "print(f\"b -> a, mappings: {align.y2x.data}\")  # array([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some insights from the alignment information generated in the example above:\n",
    "\n",
    "<ul class=\"_0c0c9282\"><li class=\"_0aa126ad\">The one-to-one mappings for the first four tokens are identical, which means\n",
    "they map to each other. This makes sense because they‚Äôre also identical in the\n",
    "input: <code class=\"_1d7c6046\">\"i\"</code>, <code class=\"_1d7c6046\">\"listened\"</code>, <code class=\"_1d7c6046\">\"to\"</code> and <code class=\"_1d7c6046\">\"obama\"</code>.</li><li class=\"_0aa126ad\">The value of <code class=\"_1d7c6046\">x2y.dataXd[6]</code> is <code class=\"_1d7c6046\">5</code>, which means that <code class=\"_1d7c6046\">other_tokens[6]</code>\n",
    "(<code class=\"_1d7c6046\">\"podcasts\"</code>) aligns to <code class=\"_1d7c6046\">spacy_tokens[5]</code> (also <code class=\"_1d7c6046\">\"podcasts\"</code>).</li><li class=\"_0aa126ad\"><code class=\"_1d7c6046\">x2y.dataXd[4]</code> and <code class=\"_1d7c6046\">x2y.dataXd[5]</code> are both <code class=\"_1d7c6046\">4</code>, which means that both tokens\n",
    "4 and 5 of <code class=\"_1d7c6046\">other_tokens</code> (<code class=\"_1d7c6046\">\"'\"</code> and <code class=\"_1d7c6046\">\"s\"</code>) align to token 4 of <code class=\"_1d7c6046\">spacy_tokens</code>\n",
    "(<code class=\"_1d7c6046\">\"'s\"</code>).</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "A Doc object‚Äôs sentences are available via the `Doc.sents` property. To view a Doc‚Äôs sentences, you can iterate over the `Doc.sents`, a generator that yields `Span` objects. You can check whether a Doc has sentence boundaries by calling `Doc.has_annotation` with the attribute name `\"SENT_START\"`.\n",
    "\n",
    "Unlike other libraries, spaCy uses the dependency parse to determine sentence boundaries. This is usually the most accurate approach, but it requires a **trained pipeline** that provides accurate predictions. If your texts are closer to general-purpose news or web text, this should work well out-of-the-box with spaCy‚Äôs provided trained pipelines. For social media or conversational text that doesn‚Äôt follow the same rules, your application may benefit from a custom trained or rule-based component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other modalities of segmentation are also available, see the [documentation](https://spacy.io/usage/linguistic-features#sbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological features\n",
    "\n",
    "Inflectional morphology is the process by which a root form of a word is modified by adding prefixes or suffixes that specify its grammatical function but do not change its part-of-speech. We say that a **lemma** (root form) is **inflected** (modified/combined) with one or more **morphological features** to create a surface form. Here are some examples:\n",
    "\n",
    "| **Context** | **Surface** | **Lemma** | **POS** | **&nbsp;Morphological Features** |\n",
    "|---|---|---|---|---|\n",
    "| I was reading the paper | reading | read | VERB | VerbForm=Ger |\n",
    "| I don‚Äôt watch the news, I read the paper | read | read | VERB | VerbForm=Fin, Mood=Ind, Tense=Pres |\n",
    "| I read the paper yesterday | read | read | VERB | VerbForm=Fin, Mood=Ind, Tense=Past |\n",
    "\n",
    "Morphological features are stored in the `token.morph` attribute of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tCase\tNumber\tPerson\tPronType\n",
      "----------------------------------------\n",
      "I\tNom\tSing\t1\tPrs\n",
      "was\t\tSing\t3\t\n",
      "reading\t\t\t\t\n",
      "the\t\t\t\tArt\n",
      "paper\t\tSing\t\t\n",
      ".\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I was reading the paper.\")\n",
    "\n",
    "def get_morphological_features(doc_or_span):\n",
    "    print(\"Token\\tCase\\tNumber\\tPerson\\tPronType\\n\" + \"-\"*40)\n",
    "    for token in doc_or_span:\n",
    "        print(\n",
    "            f\"{token.text}\\t{next(iter(token.morph.get('Case')), '')}\\t\"\n",
    "            f\"{next(iter(token.morph.get('Number')), '')}\\t\"\n",
    "            f\"{next(iter(token.morph.get('Person')), '')}\\t\"\n",
    "            f\"{next(iter(token.morph.get('PronType')), '')}\"\n",
    "        )\n",
    "\n",
    "get_morphological_features(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For languages with relatively simple morphological systems like English, spaCy can assign morphological features through a rule-based approach, which uses the token text and **fine-grained part-of-speech** tags to produce **coarse-grained part-of-speech tags** and morphological features. For other languages with a more complex morphological system, spaCy's [`Morphologizer`](https://spacy.io/api/morphologizer) is used instead.\n",
    "\n",
    "Let's now try another example using a small German pipeline including the statistical `Morphologizer` component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tCase\tNumber\tPerson\tPronType\n",
      "----------------------------------------\n",
      "Wo\t\t\t\tInt\n",
      "bist\t\tSing\t2\t\n",
      "du\tNom\tSing\t2\tPrs\n",
      "?\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp(\"Wo bist du?\") # English: 'Where are you?'\n",
    "get_morphological_features(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and similarity\n",
    "\n",
    "Remember the assignment of finding the most relevant context to a specific question last week? Similarity is determined by comparing word vectors or ‚Äúword embeddings‚Äù, multi-dimensional meaning representations of a word. Word vectors can be generated using algorithms like `word2vec` and models like `BERT`and usually look like this:\n",
    "\n",
    "```python\n",
    "array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "```\n",
    "\n",
    "The medium and large pipeline packages (that is, for example, `en_core_web_md` and `en_core_web_lg` as opposed to `en_core_web_sm`) provide word vectors for the entire vocabulary of the model. Small pipelines do not contain word vectors.\n",
    "\n",
    "Pipeline packages that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an average of their token vectors.\n",
    "\n",
    "Similarly to what we achieved in the previous lesson, we can compute a similarity score between sentences using the `similarity` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ship was traveling alonside the river. <-> The boat sailed next to the river bank. 0.8644729105970773\n",
      "The ship was traveling alonside the river. <-> I like to make money. 0.642535314700431\n",
      "The boat sailed next to the river bank. <-> I like to make money. 0.7221674694573358\n",
      "make money <-> bank 0.5089183\n",
      "make money <-> river 0.2408454\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use to install the package!\n",
    "doc1 = nlp(\"The ship was traveling alonside the river.\")\n",
    "doc2 = nlp(\"The boat sailed next to the river bank.\")\n",
    "doc3 = nlp(\"I like to make money.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "print(doc1, \"<->\", doc3, doc1.similarity(doc3))\n",
    "print(doc2, \"<->\", doc3, doc2.similarity(doc3))\n",
    "\n",
    "# Similarity of tokens and spans\n",
    "make_money = doc3[3:5]\n",
    "bank = doc2[7]\n",
    "river = doc2[6]\n",
    "print(make_money, \"<->\", bank, make_money.similarity(bank))\n",
    "print(make_money, \"<->\", river, make_money.similarity(river))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in this example how the similarity score is high for the first two sentences because of a clear semantic overlap: both sentences contain `river`, and `sailing`/`traveling` and `boat`/`ship`/`river` are probably close in embedding space.\n",
    "\n",
    "When comparing the first and last example, we can see the similarity is much lower, since they don't share common words or semantics. However, the third example shows a high similarity with the second one: this is most likely due to the presence of the word `bank`, that is semantically related to `money`. This is confirmed by comparing the similarity of `make money` with `river` and `bank` directly.\n",
    "\n",
    ">**üí° Interesting Fact** This is due to the usage of **static word vectors** like word2vec, which are pre-trained and fixed for all token in the vocabulary. Tokens exhibiting **polysemy** (i.e. multiple meanings) like `bank` will have all their possible meanings \"crammed\" into a single vector. This is one major limitation of static word vectors, and a reason why **contextualized word vectors** produced by pretrained neural language models, in which each resulting output embedding is generated dynamically depending on the full context, have gained much traction in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other references\n",
    "\n",
    "We saw just a little part of what spaCy can do. In the next week, we are going to have a better look at the tagging and parsing components of the library. These are the general functionalities of the library:\n",
    "\n",
    "| **Name** | **Description** |\n",
    "|---|---|\n",
    "| Tokenization | Segmenting text into words, punctuations marks etc. |\n",
    "| Part-of-speech (POS) Tagging | Assigning word types to tokens, like verb or noun. |\n",
    "| Dependency Parsing | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object. |\n",
    "| Lemmatization | Assigning the base forms of words. For example, the lemma of ‚Äúwas‚Äù is ‚Äúbe‚Äù, and the lemma of ‚Äúrats‚Äù is ‚Äúrat‚Äù. |\n",
    "| Sentence Boundary Detection (SBD) | Finding and segmenting individual sentences. |\n",
    "| Named Entity Recognition (NER) | Labelling named ‚Äúreal-world‚Äù objects, like persons, companies or locations. |\n",
    "| Entity Linking (EL) | Disambiguating textual entities to unique identifiers in a knowledge base. |\n",
    "| Similarity | Comparing words, text spans and documents and how similar they are to each other. |\n",
    "| Text Classification | Assigning categories or labels to a whole document, or parts of a document. |\n",
    "| Rule-based Matching | Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions. |\n",
    "| Training | Updating and improving a statistical model‚Äôs predictions. |\n",
    "| Serialization | Saving objects to files or byte strings. |\n",
    "\n",
    "Refer to [spaCy 101](https://spacy.io/usage/spacy-101) and the documentation for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling More Languages with Stanza\n",
    "\n",
    "spaCy is a great tool for English, but what if you want to use it for other languages? The good news is that spaCy supports a wide range of languages, but the bad news is that it doesn't support all of them. In this section, we will see how to understand whether a language is supported, and what to do if it isn't.\n",
    "\n",
    "The list of supported languages is available on the [spaCy website](https://spacy.io/usage/models#languages). If no packages are available for the language you are interested in, you can still use spaCy to process text in that language, but you will have to use the `Language` class directly, and you won't be able to use the pre-trained statistical models. This limits the functionality of spaCy, but it is still possible to use it for tokenization, lemmatization, and other basic NLP tasks.\n",
    "\n",
    "An alternative is to use the [Stanza](https://stanfordnlp.github.io/stanza/) library, which is a Python wrapper for the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) library. Stanza supports a wide range of languages, including Chinese, Japanese, and Arabic (full list [here](https://stanfordnlp.github.io/stanza/available_models.html)). Conveniently, Stanza authors thoroughly report the [performance](https://stanfordnlp.github.io/stanza/performance.html) of their models, so you can easily decide whether to use them or not.\n",
    "\n",
    "Stanza `Pipeline` is very similar to spaCy `Language`, and it can be used in a similar way. Notice that while we had to manually download pipelines for spaCy at the beginning of the notebook, Stanza pipelines are automatically downloaded when you first use them. Here are some examples matching the ones we saw for spaCy:\n",
    "\n",
    "**Word and Sentence Tokenization, Multiword Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 15:56:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 4.70MB/s]                    \n",
      "2023-02-03 15:56:33 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-03 15:56:33 INFO: Use device: gpu\n",
      "2023-02-03 15:56:33 INFO: Loading: tokenize\n",
      "2023-02-03 15:56:33 INFO: Done loading processors!\n",
      "2023-02-03 15:56:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: test\n",
      "id: (5,)\ttext: sentence\n",
      "id: (6,)\ttext: for\n",
      "id: (7,)\ttext: stanza\n",
      "id: (8,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: another\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 4.77MB/s]                    \n",
      "2023-02-03 15:56:33 WARNING: Language fr package default expects mwt, which has been added\n",
      "2023-02-03 15:56:33 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-02-03 15:56:33 INFO: Use device: gpu\n",
      "2023-02-03 15:56:33 INFO: Loading: tokenize\n",
      "2023-02-03 15:56:33 INFO: Loading: mwt\n",
      "2023-02-03 15:56:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: 1\tword: √áa\ttoken: √áa\n",
      "id: 2\tword: n'\ttoken: n'\n",
      "id: 3\tword: est\ttoken: est\n",
      "id: 4\tword: pas\ttoken: pas\n",
      "id: 5\tword: de\ttoken: du\n",
      "id: 6\tword: le\ttoken: du\n",
      "id: 7\tword: tout\ttoken: tout\n",
      "id: 8\tword: une\ttoken: une\n",
      "id: 9\tword: phrase\ttoken: phrase\n",
      "id: 10\tword: .\ttoken: .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')\n",
    "\n",
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize')\n",
    "doc = nlp(\"√áa n'est pas du tout une phrase.\")\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {word.id}\\tword: {word.text}\\ttoken: {word.parent.text}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging and Morphological Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 15:53:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 1.58MB/s]                    \n",
      "2023-02-03 15:53:22 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-02-03 15:53:22 INFO: Use device: gpu\n",
      "2023-02-03 15:53:22 INFO: Loading: tokenize\n",
      "2023-02-03 15:53:22 INFO: Loading: mwt\n",
      "2023-02-03 15:53:22 INFO: Loading: pos\n",
      "2023-02-03 15:53:23 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ceci\tupos: PRON\txpos: None\tfeats: Gender=Masc|Number=Sing|Person=3|PronType=Dem\n",
      "word: n‚Äô\tupos: ADV\txpos: None\tfeats: Polarity=Neg\n",
      "word: est\tupos: AUX\txpos: None\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "word: pas\tupos: ADV\txpos: None\tfeats: Polarity=Neg\n",
      "word: une\tupos: DET\txpos: None\tfeats: Definite=Ind|Gender=Fem|Number=Sing|PronType=Art\n",
      "word: pipe\tupos: NOUN\txpos: None\tfeats: Gender=Fem|Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: None\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos')\n",
    "doc = nlp('Ceci n‚Äôest pas une pipe.')\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 15:58:22 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 17.1MB/s]                    \n",
      "2023-02-03 15:58:23 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-02-03 15:58:23 INFO: Use device: gpu\n",
      "2023-02-03 15:58:23 INFO: Loading: tokenize\n",
      "2023-02-03 15:58:23 INFO: Loading: ner\n",
      "2023-02-03 15:58:23 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Gabriele Sarti\ttype: PERSON\n",
      "entity: the University of Groningen\ttype: ORG\n",
      "entity: Netherlands\ttype: GPE\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(\"Gabriele Sarti teaches at the University of Groningen. He lives in the Netherlands.\")\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find more usage examples in the [official documentation](https://stanfordnlp.github.io/stanza/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ik-nlp-tutorials-fnrzgbw7-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "926148ef506b27e4c5320622dec51791a789aa0e853a66d407eab8b330b15f1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
