{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W4E_NonTextual_Information.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in Colab to install local packages\n",
    "!pip install spacy transformers sentencepiece datasets scikit-learn pandas\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Textual and Non-textual Features in NLP Models\n",
    "\n",
    "*Based on the Text Classification tutorial by [Debora Nozza](https://dnozza.github.io/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world applications, text is just one of the multiple sources of information that can be user to predict desired quantities. In this exercise, you will reproduce a standard machine learning pipeline integrating text and non-textual information. Importantly, while in the previous tutorials and exercises we focused on the usage of advanced tooling such as the Transformers library, here we will start from the basics to establish some baseline results using the popular [Scikit-learn](https://scikit-learn.org/stable/index.html) library. \n",
    "\n",
    "**Exercise 1**, which is mandatory and will be part of your graded midterm portfolio, will include the following steps:\n",
    "\n",
    "1. Preprocess the text to extract lemmatized content words.\n",
    "\n",
    "2. Converting the text to a vector representation using simple count-based approaches.\n",
    "\n",
    "3. Convert categorical features into one-hot vectors.\n",
    "\n",
    "4. Fit a simple model to predict the desired target.\n",
    "\n",
    "5. Establish a simple baseline performance for the prediction task.\n",
    "\n",
    "6. Evaluate the model performance on a held-out set.\n",
    "\n",
    "7. Perform a feature selection and re-evaluate the model\n",
    "\n",
    "8. Obtain insights about salient words and features for the prediction task.\n",
    "\n",
    "Every operation to be completed is marked with a `TODO` comment in the code section. While these represents a small but nonetheless comprehensive set of steps to train models on textual and non-textual information, nowadays NLP practitioners operate mainly with pre-trained word embeddings for representing textual information. \n",
    "\n",
    "In **Exercise 2** (optional), you will be asked to replace our text representation with a modern transformer-based model and evaluate the difference with respect to previous results. As always, we recommend you to complete this optional exercise, especially if you plan to deal with non-textual data (e.g. quality estimation scores, translator's behavioral data) during your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: A Simple Wine Scoring Pipeline\n",
    "\n",
    "In this exercise, we will use a filtered version of the [Winemag dataset](https://www.kaggle.com/zynicide/wine-reviews) containing a collection of wine reviews (`description`), accompanied by some metadata: `country` and `provice` of provenance, `variety` of wine, `price` per bottle and the WineEnthusiast rating (`points`) describing the wine quality.\n",
    "\n",
    "*Your final goal is to build and evaluate a simple linear regression model that predicts the `points` assigned to a wine given its `description` and its other features.* This is commonly know as a **regression problem**, since you are trying to predict a continuous quantity, as opposed to a discrete one (e.g. a class label).\n",
    "\n",
    "Most importantly, the general procedure and methods you will use can be apply to any kind of data with the adequate preprocessing, and can be extended to other tasks such as binary and multiclass classification.\n",
    "\n",
    "You can have a look at the data, which has been conveniently packed into a Huggingface Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration GroNLP--ik-nlp-22_winemag-0f995f6990ce8262\n",
      "Reusing dataset csv (/home/gsarti/.cache/huggingface/datasets/csv/GroNLP--ik-nlp-22_winemag-0f995f6990ce8262/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 513.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 70458\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129857</td>\n",
       "      <td>US</td>\n",
       "      <td>Dusty tannins make for a soft texture in this ...</td>\n",
       "      <td>90</td>\n",
       "      <td>44.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Merlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112217</td>\n",
       "      <td>US</td>\n",
       "      <td>Sweet-tart Maraschino cherry and bitter brambl...</td>\n",
       "      <td>85</td>\n",
       "      <td>14.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114216</td>\n",
       "      <td>France</td>\n",
       "      <td>A lightly orange-colored rosé that is made by ...</td>\n",
       "      <td>92</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37808</td>\n",
       "      <td>France</td>\n",
       "      <td>A ripe wine that is almost off dry, this has a...</td>\n",
       "      <td>85</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31157</td>\n",
       "      <td>US</td>\n",
       "      <td>Crisp and very floral, this is a beautiful sho...</td>\n",
       "      <td>92</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index country                                        description  points  \\\n",
       "0  129857      US  Dusty tannins make for a soft texture in this ...      90   \n",
       "1  112217      US  Sweet-tart Maraschino cherry and bitter brambl...      85   \n",
       "2  114216  France  A lightly orange-colored rosé that is made by ...      92   \n",
       "3   37808  France  A ripe wine that is almost off dry, this has a...      85   \n",
       "4   31157      US  Crisp and very floral, this is a beautiful sho...      92   \n",
       "\n",
       "   price    province                   variety  \n",
       "0   44.0  California                    Merlot  \n",
       "1   14.0    New York                Pinot Noir  \n",
       "2   90.0   Champagne           Champagne Blend  \n",
       "3   17.0    Bordeaux  Bordeaux-style Red Blend  \n",
       "4   20.0  California                Pinot Gris  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"GroNLP/ik-nlp-22_winemag\")\n",
    "print(data)\n",
    "data[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Text is messy. The goal of preprocessing is to reduce the amount of noise (= unnecessary variation), while maintaining the signal. There is no one-size-fits-all solution, but a good approximation can be, for example, to preserve only content words and reduce the size of the vocabulary by means of a lemmatizer.\n",
    "\n",
    "You learned how to extract lemmas and POS tags using spaCy, and how to use `.map` to apply a function to a `Dataset`, so you should have all the tools to succeed in this. Fill in the missing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Disable unused components\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Reduce text to lower-case lemmatized content words.'''\n",
    "    content_op = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']\n",
    "    # TODO: Extract the lemmas for content words only\n",
    "    lemmas = None\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "clean_text('This is a test sentence. And here comes another one... Go me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply this cleaning function to the `description` column of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in data.keys():\n",
    "    # TODO: Use .map to apply the clean_text function to the\n",
    "    # description column, mapping the output to a new clean_text column\n",
    "    # and removing the original description column.\n",
    "    data[split] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Text\n",
    "\n",
    "Now that you have a more compact representation of the text, the next step is converting it into a vector representation. For this purpose, you will use the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class provided by Scikit-learn. This class converts a collection of text documents to a matrix of [TF-IDF scores](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) reflecting the importance of a word in a document, and in relation to the full corpus. We are going to set some parameters to ensure a limited size of the vocabulary, but you can experiment with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2), # Use 1-grams and 2-grams.\n",
    "    min_df=0.001,      # Ignore terms that appear in less than 0.1% of the documents.\n",
    "    max_df=0.75,       # Ignore terms that appear in more than 75% of documents.\n",
    "    max_features=1000,  # Use only the top 1000 most frequent words.\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# TODO: Apply the vectorizer to the clean_text column of each data split \n",
    "# by converting the Dataset object to pandas and using .fit_transform\n",
    "# Remember a Dataset object can be converted to Pandas at any\n",
    "# time by calling .to_pandas()\n",
    "text_vectors = None\n",
    "\n",
    "# Converting the text vectors to a pandas dataframe\n",
    "# Every column is a word, e.g. w_wine, w_glass, etc.\n",
    "text_vectors = pd.DataFrame(\n",
    "    text_vectors,\n",
    "    columns=[\"w_\" + w for w in vectorizer.get_feature_names()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical -> One-Hot Conversion\n",
    "\n",
    "Many of the available features in the datasets are categorical, and you will need to convert them to [one-hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) in order to use them in a regression model. Luckily, the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) class is readily available in Scikit-learn for this purpose. An even more convenient approach is to use the [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function, which returns a pandas Dataframe with labeled one-hot encoded columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Encode the fields `country`, `province` and `variety`\n",
    "# of each data split separately into one-hot vectors \n",
    "# using pd.get_dummies().\n",
    "country_vectors = None\n",
    "province_vectors = None\n",
    "variety_vectors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together and Fitting a Model\n",
    "\n",
    "Now that all the data is ready to be process, create two Pandas dataframes to train the model: `features` should be the concatenation of the `price` field plus all the vectorized features (`text_vectors`, `country_vectors`, `province_vectors`, `variety_vectors`), while `target` should be a single column of `points` field.\n",
    "\n",
    "Finally, you will train a simple linear model using the `fit` method of an instance of the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model, which fits a regular least-squares linear regression to the features. A regression model is simply a function that takes a set of numeric values, called **features**, as input, and returns an output score. Fitting a model is the process of finding the right parameters, called **weights**, to map the input features to the output targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the features and target dataframes for the train split\n",
    "features = None\n",
    "target = None\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression(n_jobs=-1)\n",
    "regressor.fit(features, target)\n",
    "print(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Baseline\n",
    "\n",
    "Before evaluating the performances of your fitted model, you might want to establish a reasonable **baseline**, representing a null-hypothesis choice. In the case of regression, usually a simple statistical baseline is the mean of the targets, minimizing the prediction error in absence any information but the distribution of target values. The Scikit-learn library implements a [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) that can be used to fit various regression baselines, including the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "baseline = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# TODO: Fit the baseline to the same data used with the regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Having a model is great, but how well does it do? Can it predict what it has seen? We need a way to estimate how well the model will work on new data. We will use two metrics: the [**mean absolute error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE), representing the mean positive prediction error across all tested instances, and the [**mean squared error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE), where the prediction error is made positive by squaring its value rather than apply the $abs$ operator. The first gives a more intuitive sense of the model's performance, while the second is more robust to outliers, which are upweighted by the squaring operation.\n",
    "\n",
    "Classifying new (held-out) data is called prediction. We reuse the weights we have learned before on a new data matrix to predict the new outcomes. \n",
    "\n",
    "**Important**: the new data needs to have the same number of features! This means using the same vectorizers you fitted on the training split, using only the `.transform` method on the test split.\n",
    "\n",
    "If you didn't apply the vectorization procedure described above to all the splits in `data`, do it now so as to obtain a `features` and `target` dataframe for each split. In the following, we will use `test_features` and `test_target` to evaluate the model using the `.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# TODO: Repeat the same steps used above for the test split\n",
    "# ...\n",
    "test_features = None\n",
    "test_target = None\n",
    "\n",
    "# TODO: Use the regressor and the baseline to predict the test target\n",
    "# using the predict method.\n",
    "regressor_predictions = None\n",
    "baseline_predictions = None\n",
    "\n",
    "# Print the scores\n",
    "for metric in [mean_absolute_error, mean_squared_error]:\n",
    "    print(\"Linear regressor\", metric.__name__, metric(test_target, regressor_predictions))\n",
    "    print(\"Mean baseline\", metric.__name__, metric(test_target, baseline_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better features = Better model\n",
    "\n",
    "We now have a lot of features! Some are simply tf-idf scores for words that will be totally unrelated to predicting the wine quality, so we might want to discard most of them. Let's select the top 500 based on how well they predict they outcome of the training data.\n",
    "\n",
    "For this purpose, you will use two classes from sklearn, [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) (the selection algorithm) and [`chi2`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) (the selection criterion). Using them in combination will allow you to remove features that are the most likely to be independent of the target, and thus not useful for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=500).fit(features, target)\n",
    "filtered_features = selector.transform(features)\n",
    "print(filtered_features.shape)\n",
    "\n",
    "# TODO: Fit another linear regression model to\n",
    "# the filtered features, and compare the performance with the\n",
    "# previous system using the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Insights about Salient Features\n",
    "\n",
    "In this exercise we used a simple linear regression model to predict the `points` of a wine given its `description` and its other features. A strenght of linear models is that they're highly interpretable: the coefficient assigned to each feature expresses the importance of the said feature in determining the predicted target. For example, a vectorized word having a large positive coefficient given by the trained regression model entails a large predicted quality score for the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the top 500 features\n",
    "top_scores = selector.scores_.argsort()[-500:]\n",
    "labels = [filtered_features.columns[i] for i in sorted(top_scores)]\n",
    "\n",
    "# TODO: Build and print a dataframe ccontaining the top 500 features\n",
    "# and their respective coefficients, sorted from highest to lowest\n",
    "# Coefficients can be accessed as regressor.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the exercise, comment on the results from the previous operation and the usefulness of textual features in predicting the `points` of a wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Exercise 2: Better Text Features for Wine Scoring\n",
    "\n",
    "In this exercise, you will repeat the same procedure of the previous exercise, but you will use a pre-trained transformer model via the Huggingface Transformers library instead of the TfidfVectorizer.\n",
    "\n",
    "Remember that extracting embeddings from pretrained models is easily done with the `pipeline(\"feature-extraction\")` class, but this can be a very time consuming process even using GPU accelerators. For this reason, consider using small models (e.g. Distilbert) and possibly select a subset of the training instances to make the process faster.\n",
    "\n",
    "**Important**: Since you are now using a model trained on naturally-occurring text, any transformation applied to text should be ignored and the original text should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reproduce the same pipeline presented above using\n",
    "# features extracted from a transformer model of your choice.\n",
    "# You will still use a Linear Regressor, only the feature from\n",
    "# step 1 will change.\n",
    "\n",
    "# TODO: Compare the performance of the new model with the original\n",
    "# models and baselines. Comment whether it is still possible to\n",
    "# understand the importance of different terms in this new setting."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85d98b63f393548f3009c8d52d8286e609a1467b1184fe464fb700873fbd3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
